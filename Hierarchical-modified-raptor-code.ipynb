{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Steps to run:- First run intra part (part 2) then inter part (part 1) and at last run part 3 (hierarchial retrieve)","metadata":{}},{"cell_type":"markdown","source":"# 1st part: Raptor Part(Inter Part)","metadata":{}},{"cell_type":"code","source":"!pip install -q umap-learn scikit-learn langchain-google-genai faiss-cpu  \n!pip install -U langchain-community\n","metadata":{"execution":{"iopub.status.busy":"2025-03-27T15:45:16.349190Z","iopub.execute_input":"2025-03-27T15:45:16.349519Z","iopub.status.idle":"2025-03-27T15:45:44.458697Z","shell.execute_reply.started":"2025-03-27T15:45:16.349488Z","shell.execute_reply":"2025-03-27T15:45:44.457818Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\ngoogle-generativeai 0.8.1 requires google-ai-generativelanguage==0.6.9, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain-community\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.49)\nCollecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.30)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.5)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.19)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community)\n  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.9.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.4)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (2.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.23.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.0)\nDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\nInstalling collected packages: httpx-sse, pydantic-settings, langchain-text-splitters, langchain, langchain-community\nSuccessfully installed httpx-sse-0.4.0 langchain-0.3.21 langchain-community-0.3.20 langchain-text-splitters-0.3.7 pydantic-settings-2.8.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\n# Try retrieving API key from environment variables\nhf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n\nif not hf_token:\n    try:\n        # If running on Kaggle, try retrieving from Kaggle Secrets\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n    except Exception:\n        raise ValueError(\"❌ Hugging Face API Key Missing! Add it to environment variables or Kaggle Secrets.\")\n\n# Perform Hugging Face Login\nlogin(token=hf_token)\nprint(\"✅ Successfully logged into Hugging Face!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:48:13.930259Z","iopub.execute_input":"2025-03-27T15:48:13.930706Z","iopub.status.idle":"2025-03-27T15:48:14.566340Z","shell.execute_reply.started":"2025-03-27T15:48:13.930673Z","shell.execute_reply":"2025-03-27T15:48:14.565448Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n✅ Successfully logged into Hugging Face!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install -q sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:49:15.371408Z","iopub.execute_input":"2025-03-27T15:49:15.372406Z","iopub.status.idle":"2025-03-27T15:49:24.220497Z","shell.execute_reply.started":"2025-03-27T15:49:15.372365Z","shell.execute_reply":"2025-03-27T15:49:24.219401Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nimport umap\nimport numpy as np\nimport faiss \nfrom sentence_transformers import SentenceTransformer\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom transformers import pipeline\n\nhf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n\n# ===  Main Inter-Document Tree Builder ===\ndef build_bottom_up_tree_inter_from_intra_roots(root_nodes_intra, max_levels=5):\n    \"\"\"\n    Builds an inter-document hierarchy (RAPTOR) using intra-document root summaries.\n    Each leaf in the inter-doc tree represents one document (its intra-root summary).\n    \"\"\"\n    import torch  # Ensure torch is available\n    all_nodes_inter = []\n    current_level_nodes = []\n    node_index_counter = 0\n\n    print(f\"\\n Using {len(root_nodes_intra)} intra roots as inter-doc leaves.\")\n\n    # Step 1: Leaf Nodes from Intra Roots\n    for doc_idx, root in enumerate(root_nodes_intra):\n        embedding = embed_text(root.summary_text)\n        node = TreeNode(\n            index=node_index_counter,\n            children=[],\n            summary_text=root.summary_text,\n            embedding=embedding,\n            parent_index=-1,\n            doc_index=doc_idx  # Set for linkage\n        )\n        all_nodes_inter.append(node)\n        current_level_nodes.append(node)\n        node_index_counter += 1\n\n    # Step 2: Hierarchical Clustering\n    for level in range(max_levels):\n        print(f\"\\n Inter-Raptor - Building level {level + 1}...\")\n\n        if len(current_level_nodes) <= 2:\n            print(\" Too few nodes left. Stopping.\")\n            break\n\n        embeddings = np.array([n.embedding for n in current_level_nodes])\n        reduced_embeddings = (\n            umap.UMAP(n_components=2, n_neighbors=5, metric=\"cosine\").fit_transform(embeddings)\n            if len(current_level_nodes) >= 10 else embeddings\n        )\n\n        n_clusters = min(len(current_level_nodes) // 2, 10)\n        if len(current_level_nodes) <= n_clusters:\n            break\n\n        gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n        labels = gmm.fit_predict(reduced_embeddings)\n\n        new_level_nodes = []\n        for cluster_id in range(n_clusters):\n            cluster_idxs = np.where(labels == cluster_id)[0]\n            if len(cluster_idxs) == 0:\n                continue\n\n            cluster_nodes = [current_level_nodes[i] for i in cluster_idxs]\n            cluster_texts = [n.summary_text for n in cluster_nodes]\n            summary = summarize_with_llm(cluster_texts)\n            summary_embedding = embed_text(summary)\n            child_indices = [n.index for n in cluster_nodes]\n\n            # Update parent index in children\n            for child in cluster_nodes:\n                child.parent_index = node_index_counter\n\n            parent_node = TreeNode(\n                index=node_index_counter,\n                children=child_indices,\n                summary_text=summary,\n                embedding=summary_embedding,\n                parent_index=-1,\n                doc_index=-1\n            )\n            all_nodes_inter.append(parent_node)\n            new_level_nodes.append(parent_node)\n            node_index_counter += 1\n\n            #  Memory cleanup\n            del summary_embedding\n            torch.cuda.empty_cache()\n\n        current_level_nodes = new_level_nodes\n        if len(current_level_nodes) == 1:\n            print(\" Inter Root Found.\")\n            break\n\n    # ===  FAISS Index\n    print(\"\\n Storing inter-document nodes in FAISS...\")\n    faiss_store_inter = FAISS.from_texts([n.summary_text for n in all_nodes_inter], hf_embeddings)\n    print(\" FAISS Inter-Document Index Built!\")\n\n    return current_level_nodes, all_nodes_inter, faiss_store_inter\n\n\n# ===  Function: Display Tree Hierarchy ===\ndef display_tree_full(root_nodes_inter, all_nodes_inter, root_nodes_intra=None, all_nodes_intra=None):\n    \"\"\"\n    Display the full tree: inter-document (Raptor) → intra-document (hierarchical) nodes.\n    Works if intra roots are indexed with doc_index for matching.\n    \"\"\"\n    index_to_node_inter = {node.index: node for node in all_nodes_inter}\n    index_to_node_intra = {node.index: node for node in all_nodes_intra} if all_nodes_intra else {}\n\n    def traverse_inter(node, depth=0):\n        indent = \"  \" * depth\n        print(f\"{indent}- [INTER] Node {node.index} → Children: {node.children}\")\n\n        # Traverse children (inter-doc Raptor tree)\n        for child_idx in node.children:\n            child_node = index_to_node_inter.get(child_idx)\n            if child_node:\n                traverse_inter(child_node, depth + 1)\n\n        # If this is a leaf inter-node and linked to intra-root\n        if not node.children and root_nodes_intra:\n            for intra_root in root_nodes_intra:\n                if getattr(intra_root, \"doc_index\", None) == node.doc_index:\n                    print(f\"{indent}  ↳ [INTRA Tree for doc {intra_root.doc_index}]\")\n                    traverse_intra(intra_root, depth + 2)\n\n    def traverse_intra(node, depth=0):\n        indent = \"  \" * depth\n        print(f\"{indent}- [INTRA] Node {node.index} → Children: {node.children}\")\n        for child_idx in node.children:\n            child_node = index_to_node_intra.get(child_idx)\n            if child_node:\n                traverse_intra(child_node, depth + 1)\n\n    for root in root_nodes_inter:\n        print(f\"\\n Inter-Doc Tree from Root Node {root.index}\")\n        traverse_inter(root)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:58:04.766206Z","iopub.execute_input":"2025-03-27T15:58:04.766540Z","iopub.status.idle":"2025-03-27T15:58:21.255773Z","shell.execute_reply.started":"2025-03-27T15:58:04.766510Z","shell.execute_reply":"2025-03-27T15:58:21.255105Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3727838802.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"root_nodes_inter, all_nodes_inter, faiss_store_inter = build_bottom_up_tree_inter_from_intra_roots(root_nodes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:58:32.772246Z","iopub.execute_input":"2025-03-27T15:58:32.772898Z","iopub.status.idle":"2025-03-27T15:58:59.492026Z","shell.execute_reply.started":"2025-03-27T15:58:32.772832Z","shell.execute_reply":"2025-03-27T15:58:59.491120Z"}},"outputs":[{"name":"stdout","text":"\n📦 Using 4 intra roots as inter-doc leaves.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26cced827b81462eb31c37e7c9762767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e898e7fea743799ad4d9ab4aae194d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15baa4779871486b9f84b2db0de1f2d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f6543a5bcf4a368635f3e63e3d7344"}},"metadata":{}},{"name":"stdout","text":"\n🔄 Inter-Raptor - Building level 1...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1771fdf350774c95bcec224bd21cf22a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd322a5812704099bccbe8bd5271ccc5"}},"metadata":{}},{"name":"stdout","text":"\n🔄 Inter-Raptor - Building level 2...\n✅ Too few nodes left. Stopping.\n\n🔄 Storing inter-document nodes in FAISS...\n✅ FAISS Inter-Document Index Built!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"display_tree_full(\n    root_nodes_inter=root_nodes_inter,\n    all_nodes_inter=all_nodes_inter,\n    root_nodes_intra=root_nodes,\n    all_nodes_intra=all_nodes\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:59:11.585021Z","iopub.execute_input":"2025-03-27T15:59:11.585369Z","iopub.status.idle":"2025-03-27T15:59:11.590674Z","shell.execute_reply.started":"2025-03-27T15:59:11.585339Z","shell.execute_reply":"2025-03-27T15:59:11.589709Z"}},"outputs":[{"name":"stdout","text":"\n🌐 Inter-Doc Tree from Root Node 4\n- [INTER] Node 4 → Children: [0, 2, 3]\n  - [INTER] Node 0 → Children: []\n  - [INTER] Node 2 → Children: []\n  - [INTER] Node 3 → Children: []\n\n🌐 Inter-Doc Tree from Root Node 5\n- [INTER] Node 5 → Children: [1]\n  - [INTER] Node 1 → Children: []\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# 2nd part: Contiguous Grouping(Intra Part)","metadata":{}},{"cell_type":"code","source":"!pip install -q chromadb > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:45:50.473240Z","iopub.execute_input":"2025-03-27T15:45:50.473661Z","iopub.status.idle":"2025-03-27T15:46:20.023896Z","shell.execute_reply.started":"2025-03-27T15:45:50.473626Z","shell.execute_reply":"2025-03-27T15:46:20.022740Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import chromadb\n\n# Correct way to initialize client with persistence\nclient = chromadb.PersistentClient(path=\"/kaggle/working/chroma_storage\")\n\n\n# Create or get collection\ncollection = client.get_or_create_collection(name=\"hierarchy_nodes\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:48:01.570338Z","iopub.execute_input":"2025-03-27T15:48:01.570695Z","iopub.status.idle":"2025-03-27T15:48:03.976406Z","shell.execute_reply.started":"2025-03-27T15:48:01.570663Z","shell.execute_reply":"2025-03-27T15:48:03.975737Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nmodel_id = \"microsoft/phi-2\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Load model with `accelerate`-based device mapping\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16,  # Reduce memory usage\n    device_map=\"auto\"  # Auto-distribute model across available devices\n)\n\n# Create text generation pipeline without specifying `device`\npipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer\n)\n\n# Load embedding model on GPU\nembedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:49:26.403128Z","iopub.execute_input":"2025-03-27T15:49:26.404014Z","iopub.status.idle":"2025-03-27T15:50:04.926654Z","shell.execute_reply.started":"2025-03-27T15:49:26.403976Z","shell.execute_reply":"2025-03-27T15:50:04.925984Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212ef473681b43f4af6ffb9cbaf23e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b52cccdf21c413cb59417e136c26375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0f7f5a3fde4d078103fc7c13bf12c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb10c89c1dd84f25bde58514cf0311e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee93987f1b9748fdae3b322a7b6903cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a2cf2f34c94561ac302304d952576b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03250d2c7b664eef937bafc1ce180abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a9b4b310244f23bbe49b3eaabefabf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf8ba60191e0414482b5b4d9286634e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31a69b68fa44c0db935438bc86cc17a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a4541984e045c5a3d1fde45513bcbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbc6629495644528e256ff38cc995ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092f75811d6b4c9d85aed5b3bffc69c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd92f81a31994446868042d4bf67d4c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7935519203374377998ecabe73e9b25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1810b4aac71a428ca4e9c9274459b529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71bbae765df048faaef7d17894f32eb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be874c9215994de2926d694dcf632f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664d380e6a3b427fbcf9a8bbc09b9114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deede888512b43699f266831a8ab6625"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8145fa5eeac54272bc5a23f901ede48f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1d51d48b714f52a439b93ac8ddb58c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1cf16afe614a748030379308087fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ed2b804adc45c8a09724f1538e5254"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:43:54.221035Z","iopub.execute_input":"2025-03-27T12:43:54.221854Z","iopub.status.idle":"2025-03-27T12:44:02.966751Z","shell.execute_reply.started":"2025-03-27T12:43:54.221820Z","shell.execute_reply":"2025-03-27T12:44:02.965889Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from langchain.document_loaders import WebBaseLoader\n\ndef load_multiple_docs(url_list):\n    loader = WebBaseLoader(url_list)\n    return loader.load()\n\n# urls = [\n#     \"https://python.langchain.com/docs/\",  # Main landing docs page\n#     \"https://python.langchain.com/docs/get_started/introduction\",  # General intro to LangChain\n#     \"https://python.langchain.com/docs/modules\",  # Overview of all major modules (chains, agents, etc.)\n#     \"https://python.langchain.com/docs/integrations\",  # All integration options\n#     \"https://python.langchain.com/docs/ecosystem\"  # LangChain ecosystem (tools, apps, etc.)\n# ]\n\nurls = [\n    \"https://python.langchain.com/docs/introduction/\",  # Intro\n    \"https://python.langchain.com/docs/tutorials/\",\n     \"https://python.langchain.com/docs/concepts/\" , # Conceptual Guide\n    \"https://python.langchain.com/docs/how_to/\"\n]\n\ndocs = load_multiple_docs(urls)\ndocs_texts = [doc.page_content for doc in docs]\n\nprint(f\"Number of docs loaded: {len(docs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:07.738591Z","iopub.execute_input":"2025-03-27T15:51:07.739265Z","iopub.status.idle":"2025-03-27T15:51:08.779599Z","shell.execute_reply.started":"2025-03-27T15:51:07.739232Z","shell.execute_reply":"2025-03-27T15:51:08.778625Z"}},"outputs":[{"name":"stdout","text":"Number of docs loaded: 4\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_docs_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n    )\n\n    all_chunks = []\n    for doc in docs:\n        chunks = splitter.split_text(doc.page_content)\n        all_chunks.append(chunks)\n\n    return all_chunks\n\nchunked_docs_2d = split_docs_into_chunks(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:12.349030Z","iopub.execute_input":"2025-03-27T15:51:12.349365Z","iopub.status.idle":"2025-03-27T15:51:12.367970Z","shell.execute_reply.started":"2025-03-27T15:51:12.349338Z","shell.execute_reply":"2025-03-27T15:51:12.367232Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"(len(chunked_docs_2d))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:15.270915Z","iopub.execute_input":"2025-03-27T15:51:15.271255Z","iopub.status.idle":"2025-03-27T15:51:15.277190Z","shell.execute_reply.started":"2025-03-27T15:51:15.271227Z","shell.execute_reply":"2025-03-27T15:51:15.276317Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"- The chunking is based on recursive splitting using specified separators ([\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]), aiming to keep chunks within a maximum length of chunk_size (e.g., 500 characters).  \n- Overlap of chunk_overlap (e.g., 100 characters) ensures contextual continuity between consecutive chunks.","metadata":{}},{"cell_type":"code","source":"from typing import List, Optional\nimport numpy as np\nimport math\nimport torch\nimport json  \n\n# Summarizer and embedding function assumed already defined\n# - summarize_with_llm(texts)\n# - embed_text(text)\n\ndef summarize_with_llm(texts):\n    prompt = \"Summarize all the important points in shortest way possible for the following text:\\n\\n\" + \"\\n\\n\".join(texts) + \"\\n\\nSummary:\"\n    with torch.no_grad():\n        # Disable gradient tracking\n        response = pipe(prompt, max_new_tokens=150, do_sample=False)  \n        \n    torch.cuda.empty_cache()  # Free up GPU memory  \n    return response[0][\"generated_text\"].strip()\n\nclass TreeNode:\n    def __init__(self, index: int, children: List[int], summary_text: str, embedding, parent_index: Optional[int] = None, doc_index: Optional[int] = None):\n        self.index = index\n        self.children = children  # list of child indices\n        self.summary_text = summary_text\n        self.embedding = embedding\n        self.parent_index = parent_index  # None if root node\n        self.doc_index = doc_index  # Only assigned for leaf nodes\n\ndef store_node_in_chroma(node: TreeNode, doc_id: int):\n    collection.add(\n        ids=[f\"doc{doc_id}_node{node.index}\"],\n        embeddings=[node.embedding],\n        documents=[node.summary_text],\n        metadatas=[{\n            \"node_index\": node.index,\n            \"doc_id\": doc_id,\n            \"children\": json.dumps(node.children),  # Convert list to JSON string\n            \"parent_index\": node.parent_index,\n            \"doc_index\": node.doc_index  \n        }]\n    )\n\ndef get_adaptive_group_size(num_chunks: int, min_group: int = 2, max_group: int = 8):\n    return max(min_group, min(max_group, int(math.log2(num_chunks + 1))))\n\n\n# Embedding function\ndef embed_text(text):\n    embedding = embedding_model.encode(text)\n    embedding = torch.tensor(embedding, dtype=torch.float16, device=\"cuda\")  # Move to GPU\n    embedding = embedding.cpu().numpy()  # Move back to CPU\n    torch.cuda.empty_cache()  # Free unused memory\n    return embedding\n    \ndef build_consecutive_hierarchy_tree(chunks: List[str], group_size: int = 2, start_index: int = 0, doc_id: int = 0):\n    \"\"\"\n    Build a hierarchy tree from consecutive text chunks (no clustering),\n    and store each node in a Chroma vector store.\n    Returns: root node index and list of all nodes\n    \"\"\"\n    all_nodes = []\n    current_level_nodes = []\n\n    print(f\"\\n Building hierarchy for Document {doc_id}\")\n    print(f\"Level 0 (leaf level): {len(chunks)} chunks\")\n\n    # Step 1: Create leaf nodes\n    for text in chunks:\n        embedding = embed_text(text)\n        node = TreeNode(index=start_index, children=[], summary_text=text, embedding=embedding, parent_index=-1, \n            doc_index=doc_id)  # Assign document index for leaf nodes)\n        all_nodes.append(node)\n        current_level_nodes.append(node)\n        \n        store_node_in_chroma(node, doc_id)  # ⬅ Store in Chroma\n        start_index += 1\n\n    # Step 2: Build higher levels by grouping consecutive nodes\n    level = 1\n    while len(current_level_nodes) > 1:\n        new_level_nodes = []\n        print(f\"Level {level}: Processing {len(current_level_nodes)} nodes → grouping into {((len(current_level_nodes)-1)//group_size)+1} nodes\")\n        for i in range(0, len(current_level_nodes), group_size):\n            group_nodes = current_level_nodes[i:i + group_size]\n            group_texts = [n.summary_text for n in group_nodes]\n            summary = summarize_with_llm(group_texts)\n            summary_embedding = embed_text(summary)\n            child_indices = [n.index for n in group_nodes]\n            parent_index = start_index  # Index of the new node being created\n\n            # Update parent index in child nodes\n            for child in group_nodes:\n                child.parent_index = parent_index\n                \n            node = TreeNode(index=parent_index, children=child_indices, summary_text=summary, embedding=summary_embedding,parent_index=-1, \n                doc_index=-1 ) # Higher-level nodes have `doc_index=None`)\n            all_nodes.append(node)\n            new_level_nodes.append(node)\n            \n            store_node_in_chroma(node, doc_id)  # ⬅ Store in Chroma\n            start_index += 1\n            \n            # Free memory after storing embeddings\n            del summary_embedding  \n            torch.cuda.empty_cache()\n            \n        current_level_nodes = new_level_nodes\n        level += 1\n\n    torch.cuda.empty_cache()\n    root_node = current_level_nodes[0]\n    print(f\" Hierarchy built for Document {doc_id} → Final root node index: {root_node.index}\\n\")\n    return root_node, all_nodes\n\ndef build_hierarchy_trees_for_documents(chunked_docs_2d: List[List[str]]):\n    \"\"\"\n    For each document (a list of chunks), build a bottom-up hierarchy tree using consecutive grouping.\n    Returns a list of root nodes and list of all nodes across all trees.\n    \"\"\"\n    all_nodes = []\n    root_nodes = []\n    node_index_counter = 0\n\n    for doc_id, doc_chunks in enumerate(chunked_docs_2d):\n        adaptive_group_size = get_adaptive_group_size(len(doc_chunks))\n        print(f\"Adaptive group size for Document {doc_id}: {adaptive_group_size}\")\n        \n        root_node, doc_nodes = build_consecutive_hierarchy_tree(doc_chunks, group_size=adaptive_group_size, start_index=node_index_counter, doc_id=doc_id)\n        root_nodes.append(root_node)\n        all_nodes.extend(doc_nodes)\n        node_index_counter = all_nodes[-1].index + 1  # update for global index\n\n        #  Clear GPU cache after each document\n        # torch.cuda.empty_cache()\n    \n    return root_nodes, all_nodes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:17.750923Z","iopub.execute_input":"2025-03-27T15:51:17.751605Z","iopub.status.idle":"2025-03-27T15:51:17.767019Z","shell.execute_reply.started":"2025-03-27T15:51:17.751574Z","shell.execute_reply":"2025-03-27T15:51:17.766283Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\n\ndef print_gpu_memory():\n    if torch.cuda.is_available():\n        print(f\"GPU Memory Usage:\")\n        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    else:\n        print(\"CUDA is not available.\")\n\nprint_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:23.594157Z","iopub.execute_input":"2025-03-27T15:51:23.594791Z","iopub.status.idle":"2025-03-27T15:51:23.600079Z","shell.execute_reply.started":"2025-03-27T15:51:23.594759Z","shell.execute_reply":"2025-03-27T15:51:23.599160Z"}},"outputs":[{"name":"stdout","text":"GPU Memory Usage:\nAllocated: 2.57 GB\nCached: 2.67 GB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"root_nodes, all_nodes = build_hierarchy_trees_for_documents(chunked_docs_2d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:51:28.371557Z","iopub.execute_input":"2025-03-27T15:51:28.372480Z","iopub.status.idle":"2025-03-27T15:56:58.029509Z","shell.execute_reply.started":"2025-03-27T15:51:28.372440Z","shell.execute_reply":"2025-03-27T15:56:58.028625Z"}},"outputs":[{"name":"stdout","text":"Adaptive group size for Document 0: 5\n\n📄 Building hierarchy for Document 0\nLevel 0 (leaf level): 35 chunks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95675d6cdfdf4e729231185e14ff0943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d89e99fc4d946b7992ba3ac28e0c455"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab793d57d83643deba8addc5f0003f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a224bba0277d4a4580c1647d26a69fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c366a78a91d4bfabb1ed398e30c5b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c077d373b55438e847b719824ae1e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e35eda8d57c4c85b92fa2f3c6c92716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca7d7d39f42429d834b950f71b79f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9c1cc9382846cba09be28027e1af66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76edf08f0f5411cb563bc331142919f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7c302ab29446edb39e534649c5ca16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f2c85f2b124618a3ef60b95e483188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c015f39f46bd4894bcb53cd881d46939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f8f88039284fbcb62b1d4745b9b502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20536becdb8488ba2977e81d83848d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5cfe032f5549c3b74f173510506557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ec2763876e47568c34603a7936644e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91619884bde74f748ee1de5f88b6d07b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18474b37bfa4f7b96c92b38d7a83707"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b55f0db977475cb335a5a67fe72769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389efd26cfb1463695e669fc82c1dbfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e10fe65c8d24a95a40b5fe1519d4e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3254db1d3e0a45c6abe5212f4cb6e9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3cf87ed9b864b18abbb8ab446d6ce9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7361f9455774543ba5baa9a974910d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f741e1ff5b4f4191baf6c003f95bb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71802c0598cd457e826aea9ce15ae178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a0ffafc01a46ed93078c742c8f8061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3950052b75dd45c6beb1338c413c937e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c7b6bb671a4f05a7d98c24bdf233f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d048ce3ffd1443f68a8d1d6000c1d070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad476c70143a4340a037f92430ed89c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aaf2d7d5461460a8ef6071c6c5854ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4efc28b6504ed1954a9a7fa45a36bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"191516485d6641b79e3a11936b428f78"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 1: Processing 35 nodes → grouping into 7 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39cf534ac99243a3aa8285d68a6fbf61"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"580f309667bf4232be0a2be3e199a34a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9d3206e62c4081bc1f15e66ce9d219"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972052da3f474743af155e9f12e0afe2"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3741a0f2b4243ebaf5af932258fa8c8"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7f027fb5204010a8733d4b3b41e2a7"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1cc3820f5834d82bb9ebfb07410db7a"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3143 > 2048). Running this sequence through the model will result in indexing errors\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 2: Processing 7 nodes → grouping into 2 nodes\n","output_type":"stream"},{"name":"stderr","text":"This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2326e21718e540b7aecfcdef4dd57973"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ee1dd6ad7d4b2d9df4f1824ec4d6c1"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 3: Processing 2 nodes → grouping into 1 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632b4ed6d0d54a1e9ae4cbfbffd68a74"}},"metadata":{}},{"name":"stdout","text":" Hierarchy built for Document 0 → Final root node index: 44\n\nAdaptive group size for Document 1: 4\n\n📄 Building hierarchy for Document 1\nLevel 0 (leaf level): 28 chunks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93bf6eb3f8e42aeac753d7708bd94e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6887fe24e4b400d92ce372a1adae6cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496fcf750ffd402999514d65839a4db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf26a7cff1246f486fe6a52a6e408bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032dcd9827cb4cebafb09d40b863d826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b11f421f8c4088a01b29f9ad043ef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e368feb84dbe4636a12e5c1f13890b4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eaa5838d5644fed90b77668fbcbe8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622d1405ef9f4be493a06cf173736559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d3b09763314037a121f355894c9bcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44fee6ca89a84d7b8073a0c7095d475f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb96dfd265c4926bde93cce364a7cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268921d80fc143e888d8a9ef3aa6cc21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ecbc271504e4826ac4e9ae2f1d44de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6897c05e609c42d0a6f98ae236cdc3b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"857ea1f4d00c4b5a9e18929c38b350a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d787fd02d44fd2b2b310e5fdd5daa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"743f4d8d4f324aa6bc99551b18eb5fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7172addb135a4cc48742b52615e3f31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528eb614ea414831a8cb5096ade5dccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b01547c0cd247e58531280bc0481c73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6856daf20e9433bac25e6708b1643d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3f7e46dfdfa4dc4817bf4fb68947eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b4dac0b99d544beb346bf7fa46711eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec14b80a09c41d6bc5c42319c7d6a20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c540fc8c780e4a639dbbfcc6cc6a9921"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c6fae1d44249299b2b97a9fff3dcec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9606536fb2a14735ad03de2dd5297119"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 1: Processing 28 nodes → grouping into 7 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3e092a81f14f13830d190534d5f2bc"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73adb9e384c24fdc8857e9a196599195"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e1570328774629b2b730281bc7a7d9"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f505e84db81945889331729604a33ecd"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb7addab4e404e8da93a12346579c0c8"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bace13b7a4a429b98889b6d5e721439"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5107c43837d74007976e0462ae5d2bc2"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 2: Processing 7 nodes → grouping into 2 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fce40d6aee24e87802b3aeb1a7b7ff1"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccd70c70fb549d7984656e24fd087d4"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 3: Processing 2 nodes → grouping into 1 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c7ba2bd6bb4364baf4fdb5137cf8a9"}},"metadata":{}},{"name":"stdout","text":" Hierarchy built for Document 1 → Final root node index: 82\n\nAdaptive group size for Document 2: 5\n\n📄 Building hierarchy for Document 2\nLevel 0 (leaf level): 46 chunks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499e89f38d1940fc8aed1f0bc47df225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8b18c6790b4bc78e74f83a41bef85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfdb1ef73884ef6b87388a22dc00ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c22aca9ab9f4002b26e90e3a8592364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42406bd025a4d50bdda0fee68c14dee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab2ddd2b7614f79b84adc51e4b73ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc53e11f67a42a8aa3c9efdba7f2d77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3f541ecb6042f3988f6a3dde208cd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7139624b6d4d07b16029a60ae7cca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0beebef431da41d6a13f154cea2dcabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd93d5fcf05b4885b185ba030f3edf70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aced55a2e90407297245fd490b8991d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e060d4ed68044a8ba6a57856a0b53a21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c8c678edfb14b6fad87449bbbf05253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0f630269f654e849cbfffa3b86305c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4caede7890240028844ebc3aa4d24a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce48671f0e86489abf967645c5dda5b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efadb0ebe841409f80f5ccca1131d8db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13cf19400e25492bad34c5fbae47360a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83cdd58ee9f541f2a2a1392e0465dce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1f971c04854c93a5b4fef28a88317f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafcc7f09c3642f0ab5185c7b9620129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939b60a0a1c1462f99b35c941f16a32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08400cdc0e5e49709e4692eaeef9358a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def07dd581564d17ae9e01c38430f231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf66d2beab246be8e056141d23c4d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b3672a377742ebbab650611d9d0bfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074fd41814b049e98a4fab1515fc90d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0592f775d6974b04a8495d07edd26b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732662bb2928479aa2ea86650ecb972c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0181dd45875c4fe688e51c9f1309cb99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de2bbb27310b4513a99947ad91b7569d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4cb7aa7ef4e4aee95f32e2beb3ed0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de22405776a741d3bcadfe8149572ca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982a92e8731c45829ca84d37b3e88666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9038dd212fbd4431a2ea5a48255d1f0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84b18c67f834cc4a192c1994f517230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb94142dcec04beab1293273166181f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6220ae08b7d445d4834ff1e3b0e525a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582ccf76092742f1b2f08ce0ad84db50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4781103db9254fe9b1662d93d1b329d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4edf1fc9cb648ab8553bc5d883ba54c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2d2c75c79947229093dcc1b7ed4250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15c498d8bd04377bfe9babcda24e8e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d76d6741144891ad3f624d449ebfd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd0d87df516466d99c7cbc35b7349f3"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 1: Processing 46 nodes → grouping into 10 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9898bbd4f3ea4313a401f26d13bd9cb6"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"020197291dcb46728d23a38b5788330d"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8ff88db1724ac3b0dbd239848b19d6"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9b92a734b04e9a8b667439f07998a5"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9048c1ddc44f11af29c99acbc931fb"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7c03f1f9814e189dbef06d2a3734e3"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720e09ab3acf464c8ad5ea021a48ddbc"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64403d4cd9544fcdba6278ed672e4868"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec95403a113c4fecb39ec354f89b7249"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4b60f99dd64afc9db6d28e2a5e4602"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 2: Processing 10 nodes → grouping into 2 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29842265b7aa42f8ae948d1f7a66db84"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4446c0deb441598986b658af9b1bb2"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 3: Processing 2 nodes → grouping into 1 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa8382efe724f6f92628533bc459732"}},"metadata":{}},{"name":"stdout","text":" Hierarchy built for Document 2 → Final root node index: 141\n\nAdaptive group size for Document 3: 5\n\n📄 Building hierarchy for Document 3\nLevel 0 (leaf level): 58 chunks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7002148c37a749d39b6939c007e4d6f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cffb00d4e6e4f84b5415d34fe5e65c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"568c7abd2fd34a7f98eccd1016f9dbad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4cc53adcd6a4f4987ce7e9f079e288d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689d40e72d67483d843f693f4812414b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad34e8a5d1cf4a0cabfab98930ce926a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1731efaf8c384f43b0bbe36782c7fb99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738fd5d097674d7fa97f78cbe5ebeb1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1e0ad545fc44b9937e52b612e667e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eabccf1dffc8407c98d9d7100893d8a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db51bd92468d4212b87305b3e25723cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71252fcfeb54e52a7d6069f810b6eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d057f465f417444cb098ea0895ab73f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd41c53c9d7490090ca4e77154c27f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9bcca7c66ae4178abe473755cabd18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65a55b93898432c94ea069636fcf9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e14f0d5e3a24cad8935613e5b469eb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b881ce7ae234324b48393d954c8a21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"014219573c774339b4b1a7e4d1076df9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222358afbd734c40ba2064fafe87ef81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542af59aed2448fbbdb0e04c2bd03df3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1072b9518114bcfb7d82412775ee5c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f510789093f49538f872324b746d28b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f175ba22157b4daab8605fd2a01f4cad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0e258663d548d58d9f3eefa248a1cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0f77cb5e134195968868f51bcde6d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63b5b265d314981aa46ff1df9687ccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace7a96d21aa43b188a9f2b5cebda98c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b18e15a28d453aa0de524e7c832d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3cb3075318542beb26511ab408130b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50ac8fcd693459aaceb92cb305fdddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09018f41723845ea91b076e28802d8c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c79f25b7eb4022b89bd516b62e8985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c38a792c3ce4d239214139acb7ee9db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4a88f8683f4770ac811e9c0329dcd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1bbe17a89b4270bfa336ac6deb2d33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a1f11f6f5584ac6902297659489ee66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf857d2b5cb40739d6a3286b7553357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca8dc5f2fc543cfbf038585a3fc8a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abadf5990b1d4e9e86a332d32f2d0f09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25dee2b96c724b079d6d9734a19e382f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e38a94533c45d9beb3b732d6b6b462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd012cb01544619a4d6fb17a23c7c04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4ae00ca4594dd0ae28976d521049ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d274edd16c184b1cb514b5c30a451ac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddde15bad2994f2ab481e4e8431c344f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c655a7dbff14a42a316a54612d5bb81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ebfd05229747f0b71c7e05d922edb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ced7bfdd1f4bbea3aa19e67dc207d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3bf73676e64812bfe4199d96e58ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"549affddab4549c38ac996dd187ff3f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ece938c6ffd4eedb976c1af8f81837b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"924d9fd663fe427aaf01d386e37d2e90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4febe06034cc4f8f977163eb46e95393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c036bcc38e4ecbb334b39f622674fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61dceb2c1abd4807b4ecd61d5418614d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce783cc23e304dcc994227894a5ce6bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c427e402854a3b8b1f95043a1e9972"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 1: Processing 58 nodes → grouping into 12 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94e88319f47d4d5c9a5d3814016dd78d"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52d912e9b2014ee092d952a62796b2fb"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654c5f4d9cde48b0bd362323d8e95316"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15ce3958b754807ac0204ae556a7a78"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"325608fe90ad4b52aa992e10ae9849db"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9487f567ce481d9ea30edabf38eaf9"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad42dc08aaa2494882d8c4f6c4be685a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf704bf5e0534def8c2f517fe1141d17"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc4a82e49d34be3807dd8fc9ca6fd59"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d3a892e7fe4472b659061e2f15e668"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c539aa5bdf4447e6b30f6356afacfe2b"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85a73891995454583bff1c17515281e"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 2: Processing 12 nodes → grouping into 3 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8edaccf11684f4db5144f5649f6bc2a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634bc150647848638cf9aa124429b869"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a22dfd555324a00a9e22c36bc2a4988"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Level 3: Processing 3 nodes → grouping into 1 nodes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17d21d4b9e442fbb47af5fe2039bce9"}},"metadata":{}},{"name":"stdout","text":" Hierarchy built for Document 3 → Final root node index: 215\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print_gpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:57:15.828403Z","iopub.execute_input":"2025-03-27T15:57:15.829189Z","iopub.status.idle":"2025-03-27T15:57:15.833802Z","shell.execute_reply.started":"2025-03-27T15:57:15.829154Z","shell.execute_reply":"2025-03-27T15:57:15.833031Z"}},"outputs":[{"name":"stdout","text":"GPU Memory Usage:\nAllocated: 2.59 GB\nCached: 2.70 GB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Print all root node indices\nprint(\"All root node indices:\", [node.index for node in root_nodes])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:57:21.158198Z","iopub.execute_input":"2025-03-27T15:57:21.158543Z","iopub.status.idle":"2025-03-27T15:57:21.163958Z","shell.execute_reply.started":"2025-03-27T15:57:21.158511Z","shell.execute_reply":"2025-03-27T15:57:21.162564Z"}},"outputs":[{"name":"stdout","text":"All root node indices: [44, 82, 141, 215]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"- Number of chunks to group\n- Chunk splititng strategy\n- Number of levels","metadata":{}},{"cell_type":"code","source":"def display_tree_from_roots(root_nodes, all_nodes):\n    \"\"\"\n    Display the tree structure from each root node.\n    Only prints node indices and their children.\n    \"\"\"\n    index_to_node = {node.index: node for node in all_nodes}\n\n    def traverse(node, depth=0):\n        indent = \"  \" * depth\n        print(f\"{indent}- Node {node.index}\")\n        for child_idx in node.children:\n            child_node = index_to_node.get(child_idx)\n            if child_node:\n                traverse(child_node, depth + 1)\n\n    for root in root_nodes:\n        print(f\"\\n Tree starting from root Node {root.index}\")\n        traverse(root)\n        \ndisplay_tree_from_roots(root_nodes, all_nodes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:57:25.263505Z","iopub.execute_input":"2025-03-27T15:57:25.263882Z","iopub.status.idle":"2025-03-27T15:57:25.272049Z","shell.execute_reply.started":"2025-03-27T15:57:25.263819Z","shell.execute_reply":"2025-03-27T15:57:25.271097Z"}},"outputs":[{"name":"stdout","text":"\n Tree starting from root Node 44\n- Node 44\n  - Node 42\n    - Node 35\n      - Node 0\n      - Node 1\n      - Node 2\n      - Node 3\n      - Node 4\n    - Node 36\n      - Node 5\n      - Node 6\n      - Node 7\n      - Node 8\n      - Node 9\n    - Node 37\n      - Node 10\n      - Node 11\n      - Node 12\n      - Node 13\n      - Node 14\n    - Node 38\n      - Node 15\n      - Node 16\n      - Node 17\n      - Node 18\n      - Node 19\n    - Node 39\n      - Node 20\n      - Node 21\n      - Node 22\n      - Node 23\n      - Node 24\n  - Node 43\n    - Node 40\n      - Node 25\n      - Node 26\n      - Node 27\n      - Node 28\n      - Node 29\n    - Node 41\n      - Node 30\n      - Node 31\n      - Node 32\n      - Node 33\n      - Node 34\n\n Tree starting from root Node 82\n- Node 82\n  - Node 80\n    - Node 73\n      - Node 45\n      - Node 46\n      - Node 47\n      - Node 48\n    - Node 74\n      - Node 49\n      - Node 50\n      - Node 51\n      - Node 52\n    - Node 75\n      - Node 53\n      - Node 54\n      - Node 55\n      - Node 56\n    - Node 76\n      - Node 57\n      - Node 58\n      - Node 59\n      - Node 60\n  - Node 81\n    - Node 77\n      - Node 61\n      - Node 62\n      - Node 63\n      - Node 64\n    - Node 78\n      - Node 65\n      - Node 66\n      - Node 67\n      - Node 68\n    - Node 79\n      - Node 69\n      - Node 70\n      - Node 71\n      - Node 72\n\n Tree starting from root Node 141\n- Node 141\n  - Node 139\n    - Node 129\n      - Node 83\n      - Node 84\n      - Node 85\n      - Node 86\n      - Node 87\n    - Node 130\n      - Node 88\n      - Node 89\n      - Node 90\n      - Node 91\n      - Node 92\n    - Node 131\n      - Node 93\n      - Node 94\n      - Node 95\n      - Node 96\n      - Node 97\n    - Node 132\n      - Node 98\n      - Node 99\n      - Node 100\n      - Node 101\n      - Node 102\n    - Node 133\n      - Node 103\n      - Node 104\n      - Node 105\n      - Node 106\n      - Node 107\n  - Node 140\n    - Node 134\n      - Node 108\n      - Node 109\n      - Node 110\n      - Node 111\n      - Node 112\n    - Node 135\n      - Node 113\n      - Node 114\n      - Node 115\n      - Node 116\n      - Node 117\n    - Node 136\n      - Node 118\n      - Node 119\n      - Node 120\n      - Node 121\n      - Node 122\n    - Node 137\n      - Node 123\n      - Node 124\n      - Node 125\n      - Node 126\n      - Node 127\n    - Node 138\n      - Node 128\n\n Tree starting from root Node 215\n- Node 215\n  - Node 212\n    - Node 200\n      - Node 142\n      - Node 143\n      - Node 144\n      - Node 145\n      - Node 146\n    - Node 201\n      - Node 147\n      - Node 148\n      - Node 149\n      - Node 150\n      - Node 151\n    - Node 202\n      - Node 152\n      - Node 153\n      - Node 154\n      - Node 155\n      - Node 156\n    - Node 203\n      - Node 157\n      - Node 158\n      - Node 159\n      - Node 160\n      - Node 161\n    - Node 204\n      - Node 162\n      - Node 163\n      - Node 164\n      - Node 165\n      - Node 166\n  - Node 213\n    - Node 205\n      - Node 167\n      - Node 168\n      - Node 169\n      - Node 170\n      - Node 171\n    - Node 206\n      - Node 172\n      - Node 173\n      - Node 174\n      - Node 175\n      - Node 176\n    - Node 207\n      - Node 177\n      - Node 178\n      - Node 179\n      - Node 180\n      - Node 181\n    - Node 208\n      - Node 182\n      - Node 183\n      - Node 184\n      - Node 185\n      - Node 186\n    - Node 209\n      - Node 187\n      - Node 188\n      - Node 189\n      - Node 190\n      - Node 191\n  - Node 214\n    - Node 210\n      - Node 192\n      - Node 193\n      - Node 194\n      - Node 195\n      - Node 196\n    - Node 211\n      - Node 197\n      - Node 198\n      - Node 199\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Create a mapping dictionary\nroot_node_mapping = {node.index: i for i, node in enumerate(root_nodes)}\n\n# Print the mapping\nprint(\"Root node mapping:\", root_node_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:59:39.902443Z","iopub.execute_input":"2025-03-27T15:59:39.902823Z","iopub.status.idle":"2025-03-27T15:59:39.907747Z","shell.execute_reply.started":"2025-03-27T15:59:39.902775Z","shell.execute_reply":"2025-03-27T15:59:39.906897Z"}},"outputs":[{"name":"stdout","text":"Root node mapping: {44: 0, 82: 1, 141: 2, 215: 3}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# 3. Hierarchical Retrieve","metadata":{}},{"cell_type":"code","source":"from typing import List\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict, deque\n\ndef cosine_sim(query_embedding, node_embedding):\n    return float(cosine_similarity([query_embedding], [node_embedding])[0][0])\n\ndef hierarchical_retrieve_combined(# uses intra and inter parts both\n    query: str,\n    root_nodes_inter: List[TreeNode],\n    all_nodes_inter: List[TreeNode],\n    root_nodes_intra: List[TreeNode],\n    all_nodes_intra: List[TreeNode],\n    similarity_threshold: float = 0.5,\n    top_k: int = 3\n):\n    # === Step 1: Embed query ===\n    query_embedding = embed_text(query)\n\n    # === Step 2: Score inter-document (Raptor) root nodes ===\n    inter_scores = [(cosine_sim(query_embedding, node.embedding), node) for node in root_nodes_inter]\n\n    print(\"\\n Inter-Document Similarity Scores:\")\n    for score, node in inter_scores:\n        print(f\"  Inter Root Node {node.index}: {score:.4f}\")\n\n    # === Step 3: Select top-k inter roots ===\n    inter_scores.sort(reverse=True, key=lambda x: x[0])\n    selected_inter_roots = [node for sim, node in inter_scores[:top_k]]\n    selected_inter_indices = set(node.index for node in selected_inter_roots)\n    print(f\"\\n Top-{top_k} Inter Root Nodes: {list(selected_inter_indices)}\")\n\n    # === Step 4: Map doc_index from inter-leaf to intra-roots ===\n    inter_leaf_doc_indices = set()\n    inter_node_map = {n.index: n for n in all_nodes_inter}\n    for root in selected_inter_roots:\n        stack = [root]\n        while stack:\n            node = stack.pop()\n            if not node.children:\n                if node.doc_index is not None:\n                    inter_leaf_doc_indices.add(node.doc_index)\n            else:\n                stack.extend(inter_node_map[c] for c in node.children)\n\n    # === Step 5: Identify matching intra-doc roots\n    matched_intra_roots = [n for n in root_nodes_intra if n.doc_index in inter_leaf_doc_indices]\n    print(f\"\\n🔗 Matching Intra Roots from Inter Leafs: {[n.index for n in matched_intra_roots]}\")\n\n    # === Step 6: Retrieve leaf nodes from intra trees\n    node_map_intra = {n.index: n for n in all_nodes_intra}\n    child_to_parent = defaultdict(list)\n    for node in all_nodes_intra:\n        for child in node.children:\n            child_to_parent[child].append(node.index)\n\n    leaf_nodes = [node for node in all_nodes_intra if not node.children and node.doc_index in inter_leaf_doc_indices]\n    selected_leaf_nodes = [n for n in leaf_nodes if cosine_sim(query_embedding, n.embedding) >= similarity_threshold]\n\n    print(f\"\\n📄 Matching Intra Leaf Nodes: {[n.index for n in selected_leaf_nodes]}\")\n    selected_leaf_nodes.sort(key=lambda x: x.index)\n    retrieved_text = \"\\n\".join([n.summary_text for n in selected_leaf_nodes])\n\n    # === Step 7: Propagate upward\n    current = set(n.index for n in selected_leaf_nodes)\n    visited = set(current)\n\n    while True:\n        parent_candidates = set()\n        for child in current:\n            parent_candidates.update(child_to_parent.get(child, []))\n\n        if not parent_candidates:\n            break\n\n        new_parents = []\n        for p in parent_candidates:\n            if p not in visited:\n                parent_node = node_map_intra[p]\n                sim = cosine_sim(query_embedding, parent_node.embedding)\n                if sim >= similarity_threshold:\n                    new_parents.append(parent_node)\n\n        if not new_parents:\n            break\n\n        print(f\"\\n Promoting Parent Nodes: {[n.index for n in new_parents]}\")\n        current = set(n.index for n in new_parents)\n        visited.update(current)\n\n    return retrieved_text\n\n## uses just intra part(initial code part, used for debugging)\ndef hierarchical_retrieve(query: str, root_nodes: List[TreeNode], all_nodes: List[TreeNode], root_node_mapping: dict, top_k: int = 3, similarity_threshold: float = 0.5):\n    # Step 1: Embed the query\n    query_embedding = embed_text(query)\n\n    # Step 2: Select top-k root nodes by cosine similarity\n    root_scores = [(cosine_sim(query_embedding, node.embedding), node) for node in root_nodes]\n\n    # Print all root node scores before sorting\n    print(\"\\n Similarity Scores for All Root Nodes:\")\n    for score, node in root_scores:\n        print(f\"Root Node {node.index}: {score:.4f}\")\n    \n    root_scores.sort(reverse=True, key=lambda x: x[0])\n    selected_roots = [node for sim, node in root_scores[:top_k]]\n    selected_root_indices = set(node.index for node in selected_roots)\n    print(f\"\\n Selected Top-{top_k} Root Nodes (similarity ≥ {similarity_threshold}): {[node.index for node in selected_roots]}\")\n\n    # Step 3: Map node index to node\n    node_map = {node.index: node for node in all_nodes}\n\n    # Step 4: Map selected root node indices to document indices\n    selected_doc_indices = set(root_node_mapping[root_index] for root_index in selected_root_indices if root_index in root_node_mapping)\n    print(f\" Mapped Root Nodes to Document Indices: {selected_doc_indices}\")\n\n    # Step 5: Build a reverse child-to-parent map\n    child_to_parent = defaultdict(list)\n    for node in all_nodes:\n        for child in node.children:\n            child_to_parent[child].append(node.index)\n\n    # Step 6: Filter eligible leaf nodes under selected root documents\n    leaf_nodes = [node for node in all_nodes if not node.children and node.doc_index in selected_doc_indices]\n\n    # Step 7: Select leaf nodes by similarity threshold\n    selected_leaf_nodes = [node for node in leaf_nodes if cosine_sim(query_embedding, node.embedding) >= similarity_threshold]\n    selected_leaf_indices = set(node.index for node in selected_leaf_nodes)\n\n    # Sort by node index for ordered output\n    selected_leaf_nodes.sort(key=lambda n: n.index)\n    retrieved_text = \"\\n\".join([n.summary_text for n in selected_leaf_nodes])\n    print(f\" Selected Leaf Nodes (under selected roots): {[n.index for n in selected_leaf_nodes]}\")\n\n    # Step 8: Upward traversal - propagate through parent levels\n    current_selected = selected_leaf_indices\n    visited = set(current_selected)\n\n    while True:\n        parent_candidates = set()\n        for child_idx in current_selected:\n            parent_idxs = child_to_parent.get(child_idx, [])\n            parent_candidates.update(parent_idxs)\n\n        if not parent_candidates:\n            print(\" No more parents found — stopping traversal.\")\n            break\n\n        parent_nodes = [node_map[i] for i in parent_candidates if i not in visited]\n        parent_scores = [(cosine_sim(query_embedding, node.embedding), node) for node in parent_nodes]\n        parent_nodes_filtered = [node for sim, node in parent_scores if sim >= similarity_threshold]\n\n        if not parent_nodes_filtered:\n            print(\" No more relevant parents (filtered by threshold) — stopping traversal.\")\n            break\n\n        print(f\" Next Level Parent Nodes: {[n.index for n in parent_nodes_filtered]}\")\n\n        current_selected = set(n.index for n in parent_nodes_filtered)\n        visited.update(current_selected)\n\n    return retrieved_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:20:22.764189Z","iopub.execute_input":"2025-03-27T16:20:22.764522Z","iopub.status.idle":"2025-03-27T16:20:22.785602Z","shell.execute_reply.started":"2025-03-27T16:20:22.764495Z","shell.execute_reply":"2025-03-27T16:20:22.784395Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"root_node_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:59:48.065044Z","iopub.execute_input":"2025-03-27T15:59:48.065903Z","iopub.status.idle":"2025-03-27T15:59:48.071247Z","shell.execute_reply.started":"2025-03-27T15:59:48.065869Z","shell.execute_reply":"2025-03-27T15:59:48.070275Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{44: 0, 82: 1, 141: 2, 215: 3}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"retrieved_text = hierarchical_retrieve(# this code is hierarchical_retrieve; uses just intra part while hierarhical retrieve combined uses intra and inter part)\n    \"How to reduce security risks in cloud platforms?\",\n    root_nodes,\n    all_nodes,\n    root_node_mapping,\n    top_k=3,\n    similarity_threshold=0.55\n)\n\nprint(\"\\n Final Retrieved Text:\\n\", retrieved_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:02:25.227339Z","iopub.execute_input":"2025-03-27T16:02:25.228259Z","iopub.status.idle":"2025-03-27T16:02:25.294255Z","shell.execute_reply.started":"2025-03-27T16:02:25.228220Z","shell.execute_reply":"2025-03-27T16:02:25.293365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b89b385acd496cbcbc767e1f7c5690"}},"metadata":{}},{"name":"stdout","text":"\n📊 Similarity Scores for All Root Nodes:\nRoot Node 44: 0.4756\nRoot Node 82: 0.4898\nRoot Node 141: 0.4783\nRoot Node 215: 0.4902\n\n📌 Selected Top-3 Root Nodes (similarity ≥ 0.55): [215, 82, 141]\n🔄 Mapped Root Nodes to Document Indices: {1, 2, 3}\n📄 Selected Leaf Nodes (under selected roots): [52, 90, 106, 149, 176, 177, 180]\n🛑 No more relevant parents (filtered by threshold) — stopping traversal.\n\n Final Retrieved Text:\n the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nthe MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nWhy LangChain?: Overview of the value that LangChain provides.\nArchitecture: How packages are organized in the LangChain ecosystem.\n\nConcepts​\nthe MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nHow to: use a vector store to retrieve data\nHow to: generate multiple queries to retrieve data for\nHow to: use contextual compression to compress the data retrieved\nHow to: write a custom retriever class\nHow to: add similarity scores to retriever results\nHow to: combine the results from multiple retrievers\nHow to: reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate metadata filters\nHow to: create a time-weighted retriever\nHow to: use hybrid vector and keyword retrieval\nHow to: return artifacts from a tool\nHow to: convert Runnables to tools\nHow to: add ad-hoc tool calling capability to models\nHow to: pass in runtime secrets\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"retrieved_text = hierarchical_retrieve_combined(\n    query=\"How to reduce security risks in cloud platforms?\",\n    root_nodes_inter=root_nodes_inter,\n    all_nodes_inter=all_nodes_inter,\n    root_nodes_intra=root_nodes,\n    all_nodes_intra=all_nodes,\n    top_k=3,\n    similarity_threshold=0.55\n)\n\nprint(\"\\n Final Retrieved Text:\\n\", retrieved_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:20:34.537354Z","iopub.execute_input":"2025-03-27T16:20:34.538182Z","iopub.status.idle":"2025-03-27T16:20:34.610610Z","shell.execute_reply.started":"2025-03-27T16:20:34.538146Z","shell.execute_reply":"2025-03-27T16:20:34.609865Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee341e0e93d49828c955abc0858cc03"}},"metadata":{}},{"name":"stdout","text":"\n🌐 Inter-Document Similarity Scores:\n  Inter Root Node 4: 0.4820\n  Inter Root Node 5: 0.4932\n\n✅ Top-3 Inter Root Nodes: [4, 5]\n\n🔗 Matching Intra Roots from Inter Leafs: []\n\n📄 Matching Intra Leaf Nodes: [7, 33, 52, 90, 106, 149, 176, 177, 180]\n\n📢 Final Retrieved Text:\n the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nAdditional resources​\nVersions​\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity​\nRead up on security best practices to make sure you're developing safely with LangChain.\nContributing​\nthe MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nthe MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nWhy LangChain?: Overview of the value that LangChain provides.\nArchitecture: How packages are organized in the LangChain ecosystem.\n\nConcepts​\nthe MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual\nHow to: use a vector store to retrieve data\nHow to: generate multiple queries to retrieve data for\nHow to: use contextual compression to compress the data retrieved\nHow to: write a custom retriever class\nHow to: add similarity scores to retriever results\nHow to: combine the results from multiple retrievers\nHow to: reorder retrieved results to mitigate the \"lost in the middle\" effect\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate multiple embeddings per document\nHow to: retrieve the whole document for a chunk\nHow to: generate metadata filters\nHow to: create a time-weighted retriever\nHow to: use hybrid vector and keyword retrieval\nHow to: return artifacts from a tool\nHow to: convert Runnables to tools\nHow to: add ad-hoc tool calling capability to models\nHow to: pass in runtime secrets\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"retrieved_text = hierarchical_retrieve_combined(\n    query=\"What is LangChain and what problems does it solve?\",\n    root_nodes_inter=root_nodes_inter,\n    all_nodes_inter=all_nodes_inter,\n    root_nodes_intra=root_nodes,\n    all_nodes_intra=all_nodes,\n    top_k=3,\n    similarity_threshold=0.70\n)\n\nprint(\"\\n Final Retrieved Text:\\n\", retrieved_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:25:57.869887Z","iopub.execute_input":"2025-03-27T16:25:57.870210Z","iopub.status.idle":"2025-03-27T16:25:57.947091Z","shell.execute_reply.started":"2025-03-27T16:25:57.870183Z","shell.execute_reply":"2025-03-27T16:25:57.946267Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b8372e08be2482498bf356b179f29a7"}},"metadata":{}},{"name":"stdout","text":"\n🌐 Inter-Document Similarity Scores:\n  Inter Root Node 4: 0.7614\n  Inter Root Node 5: 0.7042\n\n✅ Top-3 Inter Root Nodes: [4, 5]\n\n🔗 Matching Intra Roots from Inter Leafs: []\n\n📄 Matching Intra Leaf Nodes: [0, 19, 21, 22, 23, 25, 26, 27, 29, 30, 31, 32, 33, 45, 64, 66, 68, 71, 72, 83, 102, 106, 109, 114, 121, 122, 142, 161, 164, 183, 190, 193, 194, 198]\n\n🔼 Promoting Parent Nodes: [129, 133, 134, 135, 136, 35, 39, 40, 41, 200, 73, 204, 77, 78, 79, 208, 210, 211]\n\n🔼 Promoting Parent Nodes: [42, 43, 139, 140, 81, 212, 214]\n\n🔼 Promoting Parent Nodes: [44, 141, 215]\n\n📢 Final Retrieved Text:\n Introduction | 🦜️🔗 LangChain\n.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory\nLangChain is a framework for developing applications powered by large language models (LLMs).\nLangChain simplifies every stage of the LLM application lifecycle:\nDevelopment: Build your applications using LangChain's open-source components and third-party integrations.\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\nLangChain implements a standard interface for large language models and related\ntechnologies, such as embedding models and vector stores, and integrates with\nhundreds of providers. See the integrations page for\nmore.\nmodel.invoke(\"Hello, world!\")\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\nArchitecture​\nThe LangChain framework consists of multiple open-source libraries. Read more in the\nArchitecture page.\nlangchain-core: Base abstractions for chat models and other components.\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\nlangchain-community: Third-party integrations that are community maintained.\nlangchain-community: Third-party integrations that are community maintained.\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\nHow-to guides​\nHere you’ll find short answers to “How do I….?” types of questions.\nThese how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference.\nHowever, these guides will help you quickly accomplish common tasks using chat models,\nvector stores, and other common LangChain components.\nCheck out LangGraph-specific how-tos here.\nConceptual guide​\nIntroductions to all the key parts of LangChain you’ll need to know! Here you'll find high level explanations of all LangChain concepts.\nFor a deeper dive into LangGraph concepts, check out this page.\nIntegrations​\nFor a deeper dive into LangGraph concepts, check out this page.\nIntegrations​\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\nIf you're looking to get up and running quickly with chat models, vector stores,\nor other LangChain components from a specific provider, check out our growing list of integrations.\nAPI reference​\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\nEcosystem​\n🦜🛠️ LangSmith​\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\n🦜🕸️ LangGraph​\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\nAdditional resources​\nVersions​\nAdditional resources​\nVersions​\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\nSecurity​\nRead up on security best practices to make sure you're developing safely with LangChain.\nContributing​\nTutorials | 🦜️🔗 LangChain\n.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\nGet started​\nFamiliarize yourself with LangChain's open-source components by building simple applications.\nIf you're looking to get started with chat models, vector stores,\nor other LangChain components from a specific provider, check out our supported integrations.\nRefer to the how-to guides for more detail on using all LangChain components.\nOrchestration​\nGet started using LangGraph to assemble LangChain components into full-featured applications.\nLangSmith​\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse LangSmith tutorials here.\nEvaluation​\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\nEvaluate your LLM application\nEdit this pageWas this page helpful?PreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\nConceptual guide | 🦜️🔗 LangChain\n.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory\nWhy LangChain?: Overview of the value that LangChain provides.\nArchitecture: How packages are organized in the LangChain ecosystem.\n\nConcepts​\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\nStreaming: LangChain streaming APIs for surfacing results as they are generated.\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\nAsync programming: The basics that one should know to use LangChain in an asynchronous context.\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\nIntegration packages: Third-party packages that integrate with LangChain.\nIntegration tests: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\ninvoke: A standard method to invoke a Runnable.\nJSON mode: Returning responses in JSON format.\nlangchain-community: Community-driven components for LangChain.\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\nlangchain: A package for higher level components (e.g., some pre-built chains).\nlanggraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\nlangserve: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\nHow-to guides | 🦜️🔗 LangChain\n.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory\nHow to: install LangChain packages\nHow to: use LangChain with different Pydantic versions\n\nKey features​\nThis highlights functionality that is core to using LangChain.\n\nHow to: return structured data from a model\nHow to: use a model to call tools\nHow to: stream runnables\nHow to: debug your LLM apps\nCustom​\nAll of LangChain components can easily be extended to support your own versions.\n\nHow to: create a custom chat model class\nHow to: create a custom LLM class\nHow to: create a custom embeddings class\nHow to: write a custom retriever class\nHow to: write a custom document loader\nHow to: write a custom output parser class\nHow to: create custom callback handlers\nHow to: define a custom tool\nHow to: dispatch custom callback events\n\nSerialization​\n\nHow to: save and load LangChain objects\nLangChain Expression Language (LCEL)​\nShould I use LCEL?LCEL is an orchestration solution. See our\nconcepts page for recommendations on when to\nuse LCEL.\nLangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.\nLCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.\nMigration guide: For migrating legacy chain abstractions to LCEL.\nLangGraph​\nLangGraph is an extension of LangChain aimed at\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\nLangGraph documentation is currently hosted on a separate site.\nYou can peruse LangGraph how-to guides here.\nLangSmith​\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nLangSmith​\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\nLangSmith documentation is hosted on a separate site.\nYou can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly\nrelevant to LangChain below:\nEvaluation​\nanalysisQ&A over SQL + CSVQ&A over graph databasesSummarizationLangChain Expression Language (LCEL)LangGraphLangSmithEvaluationTracingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc\n","output_type":"stream"}],"execution_count":34}]}